{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHXTOzE29vhy",
        "outputId": "a0b13896-12a3-4729-8509-297f910995ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sample fake news detection dataset...\n",
            "Dataset created with 18 articles:\n",
            "label\n",
            "1    10\n",
            "0     8\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# importing required modules\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download  NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "except:\n",
        "    print(\"NLTK downloads completed or already available\")\n",
        "\n",
        "# Create a sample dataset for demo\n",
        "print(\"Creating sample fake news detection dataset...\")\n",
        "\n",
        "# Sample news articles with labels (0 = real, 1 = fake)\n",
        "sample_data = [\n",
        "    # Real news examples\n",
        "    (\"The stock market opened higher today as investors responded positively to economic indicators.\", 0),\n",
        "    (\"Scientists at Harvard University have published a new study on climate change effects.\", 0),\n",
        "    (\"The Federal Reserve announced its decision to maintain interest rates at current levels.\", 0),\n",
        "    (\"A new medical breakthrough in cancer treatment has been reported by researchers.\", 0),\n",
        "    (\"The president signed a new bill into law yesterday afternoon during a ceremony.\", 0),\n",
        "    (\"Weather forecasters predict heavy rainfall across the midwest region this weekend.\", 0),\n",
        "    (\"Local authorities reported a decrease in crime rates for the third consecutive month.\", 0),\n",
        "    (\"Technology companies are investing heavily in renewable energy infrastructure.\", 0),\n",
        "\n",
        "    # Fake news examples (more sensational, emotional language)\n",
        "    (\"SHOCKING: Secret government conspiracy exposed! They don't want you to know this!\", 1),\n",
        "    (\"You won't believe what this celebrity said about politics - liberals are furious!\", 1),\n",
        "    (\"BREAKING: Miracle cure for all diseases discovered but Big Pharma is hiding it!\", 1),\n",
        "    (\"URGENT: This one simple trick will make you rich overnight - bankers hate this!\", 1),\n",
        "    (\"EXCLUSIVE: Aliens confirmed by government officials - full disclosure imminent!\", 1),\n",
        "    (\"WARNING: Popular food item causes instant death - remove from your home now!\", 1),\n",
        "    (\"BOMBSHELL: Political opponent caught in massive scandal - career ending photos!\", 1),\n",
        "    (\"AMAZING: Local woman loses 50 pounds in one week using this weird trick!\", 1),\n",
        "    (\"CRISIS: Economic collapse predicted for next month - prepare for chaos now!\", 1),\n",
        "    (\"REVEALED: Secret agenda to control your mind through everyday products!\", 1)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(sample_data, columns=['text', 'label'])\n",
        "print(f\"Dataset created with {len(df)} articles:\")\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing functions\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        # Download necessary NLTK data\n",
        "        try:\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            nltk.download('stopwords', quiet=True)\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "            nltk.download('punkt_tab', quiet=True)\n",
        "        except Exception as e:\n",
        "            print(f\"NLTK download failed: {e}\")\n",
        "\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove common stopwords\"\"\"\n",
        "        return [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "    def lemmatize_text(self, tokens):\n",
        "        \"\"\"Apply lemmatization to reduce words to root form\"\"\"\n",
        "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    def preprocess_pipeline(self, text):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = self.tokenize_text(cleaned_text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "\n",
        "        # Lemmatize\n",
        "        tokens = self.lemmatize_text(tokens)\n",
        "\n",
        "        # Rejoin tokens\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Initialize preprocessor and apply to dataset\n",
        "preprocessor = TextPreprocessor()\n",
        "print(\"Applying text preprocessing...\")\n",
        "\n",
        "# Apply preprocessing\n",
        "df['cleaned_text'] = df['text'].apply(preprocessor.preprocess_pipeline)\n",
        "\n",
        "# Show examples of preprocessing\n",
        "print(\"\\nPreprocessing examples:\")\n",
        "for i in range(min(3, len(df))): # Ensure we don't go out of bounds for small datasets\n",
        "    print(f\"Original: {df.iloc[i]['text'][:80]}...\")\n",
        "    print(f\"Cleaned:  {df.iloc[i]['cleaned_text'][:80]}...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(f\"Preprocessing completed. Sample cleaned text lengths: {df['cleaned_text'].str.len().describe()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-SWodjW92zR",
        "outputId": "ed0c43aa-c69d-41f0-a9dc-188b686e9ad9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying text preprocessing...\n",
            "\n",
            "Preprocessing examples:\n",
            "Original: The stock market opened higher today as investors responded positively to econom...\n",
            "Cleaned:  stock market opened higher today investor responded positively economic indicato...\n",
            "--------------------------------------------------\n",
            "Original: Scientists at Harvard University have published a new study on climate change ef...\n",
            "Cleaned:  scientist harvard university published new study climate change effect...\n",
            "--------------------------------------------------\n",
            "Original: The Federal Reserve announced its decision to maintain interest rates at current...\n",
            "Cleaned:  federal reserve announced decision maintain interest rate current level...\n",
            "--------------------------------------------------\n",
            "Preprocessing completed. Sample cleaned text lengths: count    18.000000\n",
            "mean     63.333333\n",
            "std       8.073195\n",
            "min      52.000000\n",
            "25%      58.000000\n",
            "50%      60.500000\n",
            "75%      70.000000\n",
            "max      81.000000\n",
            "Name: cleaned_text, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative preprocessing approach\n",
        "import nltk\n",
        "try:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Alternative text preprocessing using simple methods\n",
        "class SimpleTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        # Define stopwords manually if NLTK fails\n",
        "        self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
        "                              'this', 'that', 'these', 'those', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
        "                              'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',\n",
        "                              'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them',\n",
        "                              'their', 'theirs', 'themselves', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "                              'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'will', 'would',\n",
        "                              'could', 'should', 'may', 'might', 'must', 'can'])\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Remove special characters and digits but keep spaces\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def simple_tokenize(self, text):\n",
        "        \"\"\"Simple tokenization by splitting on spaces\"\"\"\n",
        "        return text.split()\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove common stopwords\"\"\"\n",
        "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "    def preprocess_pipeline(self, text):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        # Simple tokenize\n",
        "        tokens = self.simple_tokenize(cleaned_text)\n",
        "\n",
        "        # Remove stopwords and short words\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "\n",
        "        # Rejoin tokens\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Initialize simple preprocessor and apply to dataset\n",
        "simple_preprocessor = SimpleTextPreprocessor()\n",
        "print(\"Applying simplified text preprocessing...\")\n",
        "\n",
        "# Apply preprocessing\n",
        "df['cleaned_text'] = df['text'].apply(simple_preprocessor.preprocess_pipeline)\n",
        "\n",
        "# Show examples of preprocessing\n",
        "print(\"\\nPreprocessing examples:\")\n",
        "for i in range(3):\n",
        "    print(f\"Original: {df.iloc[i]['text']}\")\n",
        "    print(f\"Cleaned:  {df.iloc[i]['cleaned_text']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\nPreprocessing completed.\")\n",
        "print(f\"Average cleaned text length: {df['cleaned_text'].str.len().mean():.1f} characters\")\n",
        "print(f\"Total unique words after cleaning: {len(set(' '.join(df['cleaned_text']).split()))}\")\n",
        "\n",
        "# Display dataset info\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Label distribution:\")\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvw_3mjW92o5",
        "outputId": "cabc7711-8726-457e-93fc-22ba89180967"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying simplified text preprocessing...\n",
            "\n",
            "Preprocessing examples:\n",
            "Original: The stock market opened higher today as investors responded positively to economic indicators.\n",
            "Cleaned:  stock market opened higher today investors responded positively economic indicators\n",
            "--------------------------------------------------------------------------------\n",
            "Original: Scientists at Harvard University have published a new study on climate change effects.\n",
            "Cleaned:  scientists harvard university published new study climate change effects\n",
            "--------------------------------------------------------------------------------\n",
            "Original: The Federal Reserve announced its decision to maintain interest rates at current levels.\n",
            "Cleaned:  federal reserve announced decision maintain interest rates current levels\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Preprocessing completed.\n",
            "Average cleaned text length: 67.1 characters\n",
            "Total unique words after cleaning: 148\n",
            "\n",
            "Dataset shape: (18, 3)\n",
            "Label distribution:\n",
            "label\n",
            "1    10\n",
            "0     8\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning Models\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class FakeNewsDetector:\n",
        "    def __init__(self):\n",
        "        self.vectorizers = {}\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def create_features(self, X_train, X_test):\n",
        "        \"\"\"Create multiple feature representations\"\"\"\n",
        "        feature_sets = {}\n",
        "\n",
        "        # 1. TF-IDF Features\n",
        "        print(\"Creating TF-IDF features...\")\n",
        "        tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words='english')\n",
        "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf.transform(X_test)\n",
        "        feature_sets['tfidf'] = (X_train_tfidf, X_test_tfidf)\n",
        "        self.vectorizers['tfidf'] = tfidf\n",
        "\n",
        "        # 2. Count Features (Bag of Words)\n",
        "        print(\"Creating Count/BoW features...\")\n",
        "        count = CountVectorizer(max_features=1000, ngram_range=(1, 2), stop_words='english')\n",
        "        X_train_count = count.fit_transform(X_train)\n",
        "        X_test_count = count.transform(X_test)\n",
        "        feature_sets['count'] = (X_train_count, X_test_count)\n",
        "        self.vectorizers['count'] = count\n",
        "\n",
        "        return feature_sets\n",
        "\n",
        "    def train_models(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train multiple ML models\"\"\"\n",
        "\n",
        "        # Create feature sets\n",
        "        feature_sets = self.create_features(X_train, X_test)\n",
        "\n",
        "        # Define models\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'Naive Bayes': MultinomialNB(),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'SVM': SVC(kernel='linear', random_state=42),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Train and evaluate models for each feature set\n",
        "        for feature_name, (X_tr, X_te) in feature_sets.items():\n",
        "            print(f\"\\n--- Training models with {feature_name.upper()} features ---\")\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                print(f\"Training {model_name}...\")\n",
        "\n",
        "                # Train model\n",
        "                model.fit(X_tr, y_train)\n",
        "\n",
        "                # Predictions\n",
        "                y_pred = model.predict(X_te)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                # Store results\n",
        "                key = f\"{model_name}_{feature_name}\"\n",
        "                self.results[key] = {\n",
        "                    'model': model,\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'predictions': y_pred\n",
        "                }\n",
        "\n",
        "                print(f\"  Accuracy: {accuracy:.3f}\")\n",
        "                print(f\"  F1-Score: {f1:.3f}\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def get_best_model(self):\n",
        "        \"\"\"Find the best performing model\"\"\"\n",
        "        best_key = max(self.results.keys(), key=lambda k: self.results[k]['f1'])\n",
        "        return best_key, self.results[best_key]\n",
        "\n",
        "# Prepare data\n",
        "X = df['cleaned_text']\n",
        "y = df['label']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Training label distribution: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Test label distribution: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# Initialize and train detector\n",
        "detector = FakeNewsDetector()\n",
        "results = detector.train_models(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HGwa6Kp97eQ",
        "outputId": "163ac925-6e51-4a85-edfb-f217c1757232"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 12\n",
            "Test set size: 6\n",
            "Training label distribution: {1: 7, 0: 5}\n",
            "Test label distribution: {0: 3, 1: 3}\n",
            "Creating TF-IDF features...\n",
            "Creating Count/BoW features...\n",
            "\n",
            "--- Training models with TFIDF features ---\n",
            "Training Logistic Regression...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training Naive Bayes...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training Random Forest...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training SVM...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training Gradient Boosting...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "\n",
            "--- Training models with COUNT features ---\n",
            "Training Logistic Regression...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training Naive Bayes...\n",
            "  Accuracy: 0.833\n",
            "  F1-Score: 0.829\n",
            "Training Random Forest...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training SVM...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n",
            "Training Gradient Boosting...\n",
            "  Accuracy: 0.500\n",
            "  F1-Score: 0.333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# more realistic dataset and implement advanced features\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate a larger synthetic dataset with more realistic examples\n",
        "def generate_fake_news_dataset(size=1000):\n",
        "    \"\"\"Generate a larger synthetic dataset for better model training\"\"\"\n",
        "\n",
        "    # Real news patterns\n",
        "    real_templates = [\n",
        "        \"Researchers at {institution} published findings on {topic}.\",\n",
        "        \"The {organization} announced {action} regarding {subject}.\",\n",
        "        \"According to {source}, {statistic} showed {trend}.\",\n",
        "        \"Officials reported {event} in {location} yesterday.\",\n",
        "        \"{Expert} stated that {topic} requires {action}.\",\n",
        "        \"A study conducted by {institution} revealed {finding}.\",\n",
        "        \"The government has decided to {action} following {event}.\",\n",
        "        \"{Company} reported {financial_news} in their latest earnings.\",\n",
        "        \"Weather forecasters predict {weather_event} for {location}.\",\n",
        "        \"Local authorities confirmed {event} affecting {number} people.\"\n",
        "    ]\n",
        "\n",
        "    # Fake news patterns (more sensational)\n",
        "    fake_templates = [\n",
        "        (\"SHOCKING: {conspiracy} exposed! {authority} trying to hide {secret}!\", [\"conspiracy\", \"authority\", \"secret\"]),\n",
        "        (\"You WON'T BELIEVE what {person} said about {topic} - {group} are FURIOUS!\", [\"person\", \"topic\", \"group\"]),\n",
        "        (\"BREAKING: {miracle} discovered but {enemy} doesn't want you to know!\", [\"miracle\", \"enemy\"]),\n",
        "        (\"URGENT WARNING: {danger} found in {common_item} - remove immediately!\", [\"danger\", \"common_item\"]),\n",
        "        (\"EXCLUSIVE: {celebrity} reveals {secret} that will change {field} FOREVER!\", [\"celebrity\", \"secret\", \"field\"]),\n",
        "        (\"BOMBSHELL: {scandal} rocks {organization} - {consequence} expected!\", [\"scandal\", \"organization\", \"consequence\"]),\n",
        "        (\"AMAZING: {person} achieves {impossible_feat} using this {method}!\", [\"person\", \"impossible_feat\", \"method\"]),\n",
        "        (\"CRISIS: {disaster} predicted for {timeframe} - experts say {warning}!\", [\"disaster\", \"timeframe\", \"warning\"]),\n",
        "        (\"SECRET REVEALED: {conspiracy} behind {event} finally exposed!\", [\"conspiracy\", \"event\"]),\n",
        "        (\"MIRACLE CURE: {treatment} eliminates {disease} in {timeframe}!\", [\"treatment\", \"disease\", \"timeframe\"])\n",
        "    ]\n",
        "\n",
        "    # Word pools for templates\n",
        "    institutions = [\"Harvard University\", \"MIT\", \"Stanford\", \"CDC\", \"WHO\", \"NASA\"]\n",
        "    topics = [\"climate change\", \"artificial intelligence\", \"medicine\", \"space exploration\", \"economics\", \"technology\"]\n",
        "    organizations = [\"Federal Reserve\", \"Department of Education\", \"EPA\", \"FDA\", \"Pentagon\"]\n",
        "    sources = [\"Reuters\", \"Associated Press\", \"Bloomberg\", \"Wall Street Journal\"]\n",
        "    locations = [\"California\", \"New York\", \"Texas\", \"Florida\", \"Washington\"]\n",
        "    companies = [\"Apple\", \"Microsoft\", \"Google\", \"Amazon\", \"Tesla\"]\n",
        "    actions = [\"announced new policies\", \"released a statement\", \"updated guidelines\"]\n",
        "    statistics = [\"recent data\", \"latest figures\", \"new research\"]\n",
        "    trends = [\"positive results\", \"concerning patterns\", \"significant changes\"]\n",
        "    events = [\"a conference\", \"new regulations\", \"policy changes\"]\n",
        "    experts = [\"Dr. Smith\", \"Professor Johnson\", \"Researcher Williams\"]\n",
        "    findings = [\"important correlations\", \"significant data\", \"new insights\"]\n",
        "    financial_news_options = [\"increased revenue\", \"quarterly growth\", \"market expansion\"]\n",
        "    weather_events = [\"heavy rainfall\", \"temperature changes\", \"storm systems\"]\n",
        "    numbers = [\"hundreds of\", \"thousands of\", \"several\"]\n",
        "\n",
        "    # Fake news specific words\n",
        "    conspiracies = [\"government mind control\", \"alien coverup\", \"big pharma conspiracy\", \"secret society plan\"]\n",
        "    authorities = [\"The elite\", \"Government officials\", \"Corporate leaders\", \"Secret agencies\"]\n",
        "    miracles = [\"Revolutionary cure\", \"Ancient secret\", \"Forbidden knowledge\", \"Hidden technology\"]\n",
        "    enemies = [\"Big Pharma\", \"The establishment\", \"Corporate elites\", \"Government agencies\"]\n",
        "    dangers = [\"deadly chemicals\", \"harmful substances\", \"toxic materials\"]\n",
        "    common_items = [\"everyday foods\", \"household products\", \"popular items\"]\n",
        "    celebrities = [\"Famous actor\", \"Popular singer\", \"Well-known politician\"]\n",
        "    fields = [\"medicine\", \"technology\", \"science\", \"politics\"]\n",
        "    scandals = [\"massive corruption\", \"shocking revelation\", \"hidden agenda\"]\n",
        "    consequences = [\"major changes\", \"immediate action\", \"public outcry\"]\n",
        "    impossible_feats = [\"amazing weight loss\", \"incredible wealth\", \"perfect health\"]\n",
        "    methods = [\"simple trick\", \"secret technique\", \"ancient method\"]\n",
        "    disasters = [\"economic collapse\", \"natural disaster\", \"social chaos\"]\n",
        "    timeframes = [\"next month\", \"this year\", \"very soon\"]\n",
        "    warnings = [\"prepare now\", \"take action\", \"stock up\"]\n",
        "    treatments = [\"natural remedy\", \"home solution\", \"simple cure\"]\n",
        "    diseases = [\"all cancers\", \"chronic pain\", \"serious illness\"]\n",
        "    people = [\"This doctor\", \"A whistleblower\", \"An insider\", \"This expert\"]\n",
        "    groups = [\"liberals\", \"conservatives\", \"experts\", \"officials\"]\n",
        "\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    # Generate real news\n",
        "    for _ in range(size // 2):\n",
        "        template = random.choice(real_templates)\n",
        "        text = template.format(\n",
        "            institution=random.choice(institutions),\n",
        "            topic=random.choice(topics),\n",
        "            organization=random.choice(organizations),\n",
        "            action=random.choice(actions),\n",
        "            subject=random.choice(topics),\n",
        "            source=random.choice(sources),\n",
        "            statistic=random.choice(statistics),\n",
        "            trend=random.choice(trends),\n",
        "            event=random.choice(events),\n",
        "            location=random.choice(locations),\n",
        "            Expert=random.choice(experts),\n",
        "            finding=random.choice(findings),\n",
        "            Company=random.choice(companies),\n",
        "            financial_news=random.choice(financial_news_options),\n",
        "            weather_event=random.choice(weather_events),\n",
        "            number=random.choice(numbers)\n",
        "        )\n",
        "        dataset.append((text, 0))  # 0 for real news\n",
        "\n",
        "    # Generate fake news\n",
        "    for _ in range(size // 2):\n",
        "        template_info = random.choice(fake_templates)\n",
        "        template = template_info[0]\n",
        "        placeholders = template_info[1]\n",
        "\n",
        "        format_dict = {}\n",
        "        for placeholder in placeholders:\n",
        "            if placeholder == \"conspiracy\":\n",
        "                format_dict[placeholder] = random.choice(conspiracies)\n",
        "            elif placeholder == \"authority\":\n",
        "                format_dict[placeholder] = random.choice(authorities)\n",
        "            elif placeholder == \"secret\":\n",
        "                format_dict[placeholder] = random.choice([\"the truth\", \"dangerous information\", \"classified data\"])\n",
        "            elif placeholder == \"person\":\n",
        "                format_dict[placeholder] = random.choice(people)\n",
        "            elif placeholder == \"topic\":\n",
        "                format_dict[placeholder] = random.choice(topics)\n",
        "            elif placeholder == \"group\":\n",
        "                format_dict[placeholder] = random.choice(groups)\n",
        "            elif placeholder == \"miracle\":\n",
        "                format_dict[placeholder] = random.choice(miracles)\n",
        "            elif placeholder == \"enemy\":\n",
        "                format_dict[placeholder] = random.choice(enemies)\n",
        "            elif placeholder == \"danger\":\n",
        "                format_dict[placeholder] = random.choice(dangers)\n",
        "            elif placeholder == \"common_item\":\n",
        "                format_dict[placeholder] = random.choice(common_items)\n",
        "            elif placeholder == \"celebrity\":\n",
        "                format_dict[placeholder] = random.choice(celebrities)\n",
        "            elif placeholder == \"field\":\n",
        "                format_dict[placeholder] = random.choice(fields)\n",
        "            elif placeholder == \"scandal\":\n",
        "                format_dict[placeholder] = random.choice(scandals)\n",
        "            elif placeholder == \"organization\":\n",
        "                 format_dict[placeholder] = random.choice(organizations) # Add organization to fake news placeholders\n",
        "            elif placeholder == \"consequence\":\n",
        "                format_dict[placeholder] = random.choice(consequences)\n",
        "            elif placeholder == \"impossible_feat\":\n",
        "                format_dict[placeholder] = random.choice(impossible_feats)\n",
        "            elif placeholder == \"method\":\n",
        "                format_dict[placeholder] = random.choice(methods)\n",
        "            elif placeholder == \"disaster\":\n",
        "                format_dict[placeholder] = random.choice(disasters)\n",
        "            elif placeholder == \"timeframe\":\n",
        "                format_dict[placeholder] = random.choice(timeframes)\n",
        "            elif placeholder == \"warning\":\n",
        "                format_dict[placeholder] = random.choice(warnings)\n",
        "            elif placeholder == \"treatment\":\n",
        "                format_dict[placeholder] = random.choice(treatments)\n",
        "            elif placeholder == \"disease\":\n",
        "                format_dict[placeholder] = random.choice(diseases)\n",
        "            elif placeholder == \"event\":\n",
        "                format_dict[placeholder] = random.choice(events)\n",
        "\n",
        "\n",
        "        text = template.format(**format_dict)\n",
        "        dataset.append((text, 1))  # 1 for fake news\n",
        "\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Create larger dataset\n",
        "print(\"Generating larger synthetic dataset...\")\n",
        "large_dataset = generate_fake_news_dataset(1000)\n",
        "df_large = pd.DataFrame(large_dataset, columns=['text', 'label'])\n",
        "\n",
        "# Shuffle the dataset\n",
        "df_large = df_large.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Large dataset created with {len(df_large)} articles:\")\n",
        "print(df_large['label'].value_counts())\n",
        "print(\"\\nSample articles:\")\n",
        "for i in range(2):\n",
        "    print(f\"Real: {df_large[df_large['label']==0].iloc[i]['text']}\")\n",
        "    print(f\"Fake: {df_large[df_large['label']==1].iloc[i]['text']}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAGh2Mpa99hO",
        "outputId": "874046e2-b9d4-46ef-b48d-a90cce819317"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating larger synthetic dataset...\n",
            "Large dataset created with 1000 articles:\n",
            "label\n",
            "1    500\n",
            "0    500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample articles:\n",
            "Real: Local authorities confirmed policy changes affecting several people.\n",
            "Fake: EXCLUSIVE: Popular singer reveals classified data that will change science FOREVER!\n",
            "--------------------------------------------------------------------------------\n",
            "Real: Officials reported policy changes in Florida yesterday.\n",
            "Fake: BREAKING: Revolutionary cure discovered but The establishment doesn't want you to know!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed dataset generation with proper template handling\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_fake_news_dataset(size=1000):\n",
        "    \"\"\"Generate a larger synthetic dataset for better model training\"\"\"\n",
        "\n",
        "    # Pre-defined content pools\n",
        "    institutions = [\"Harvard University\", \"MIT\", \"Stanford\", \"CDC\", \"WHO\", \"NASA\"]\n",
        "    topics = [\"climate change\", \"artificial intelligence\", \"medicine\", \"space exploration\", \"economics\", \"technology\"]\n",
        "    organizations = [\"Federal Reserve\", \"Department of Education\", \"EPA\", \"FDA\", \"Pentagon\"]\n",
        "    sources = [\"Reuters\", \"Associated Press\", \"Bloomberg\", \"Wall Street Journal\"]\n",
        "    locations = [\"California\", \"New York\", \"Texas\", \"Florida\", \"Washington\"]\n",
        "    companies = [\"Apple\", \"Microsoft\", \"Google\", \"Amazon\", \"Tesla\"]\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    # Generate real news (500 articles)\n",
        "    real_patterns = [\n",
        "        f\"Researchers at {random.choice(institutions)} published findings on {random.choice(topics)}.\",\n",
        "        f\"The {random.choice(organizations)} announced new policies regarding {random.choice(topics)}.\",\n",
        "        f\"According to {random.choice(sources)}, recent data showed positive trends.\",\n",
        "        f\"Officials reported policy changes in {random.choice(locations)} yesterday.\",\n",
        "        f\"Dr. Smith stated that {random.choice(topics)} requires careful consideration.\",\n",
        "        f\"A study conducted by {random.choice(institutions)} revealed important insights.\",\n",
        "        f\"The government has decided to update regulations following recent events.\",\n",
        "        f\"{random.choice(companies)} reported quarterly growth in their latest earnings.\",\n",
        "        f\"Weather services predict storm systems for {random.choice(locations)}.\",\n",
        "        f\"Local authorities confirmed infrastructure improvements affecting thousands.\"\n",
        "    ]\n",
        "\n",
        "    for _ in range(size // 2):\n",
        "        text = random.choice(real_patterns)\n",
        "        dataset.append((text, 0))\n",
        "\n",
        "    # Generate fake news\n",
        "    fake_patterns = [\n",
        "        \"SHOCKING: Government mind control exposed! Officials trying to hide the truth!\",\n",
        "        \"You WON'T BELIEVE what this doctor said about vaccines - experts are FURIOUS!\",\n",
        "        \"BREAKING: Revolutionary cure discovered but Big Pharma doesn't want you to know!\",\n",
        "        \"URGENT WARNING: Deadly chemicals found in everyday foods - remove immediately!\",\n",
        "        \"EXCLUSIVE: Famous actor reveals secret that will change medicine FOREVER!\",\n",
        "        \"BOMBSHELL: Massive corruption rocks Pentagon - major changes expected!\",\n",
        "        \"AMAZING: Local woman achieves incredible wealth using this simple trick!\",\n",
        "        \"CRISIS: Economic collapse predicted for next month - experts say prepare now!\",\n",
        "        \"SECRET REVEALED: Alien coverup behind space exploration finally exposed!\",\n",
        "        \"MIRACLE CURE: Natural remedy eliminates all cancers in one week!\",\n",
        "        \"SHOCKING DISCOVERY: Ancient secret eliminates chronic disease instantly!\",\n",
        "        \"BREAKING NEWS: Hidden technology suppressed by corporate elites revealed!\",\n",
        "        \"URGENT ALERT: Popular household products contain dangerous toxins!\",\n",
        "        \"EXCLUSIVE REPORT: Government agencies hiding cure for serious illness!\",\n",
        "        \"AMAZING BREAKTHROUGH: Simple home remedy reverses aging process!\",\n",
        "        \"BOMBSHELL REVELATION: Secret society plan exposed by whistleblower!\",\n",
        "        \"INCREDIBLE: This forbidden knowledge will make you wealthy overnight!\",\n",
        "        \"WARNING: Big Pharma conspiracy to suppress natural treatments!\",\n",
        "        \"SHOCKING: Climate change is hoax designed to control population!\",\n",
        "        \"BREAKING: Vaccines contain mind control chips - scientists confirm!\"\n",
        "    ]\n",
        "\n",
        "    for _ in range(size // 2):\n",
        "        text = random.choice(fake_patterns)\n",
        "        dataset.append((text, 1))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Create larger dataset\n",
        "print(\"Generating larger synthetic dataset...\")\n",
        "large_dataset = generate_fake_news_dataset(1000)\n",
        "df_large = pd.DataFrame(large_dataset, columns=['text', 'label'])\n",
        "\n",
        "# Shuffle the dataset\n",
        "df_large = df_large.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Large dataset created with {len(df_large)} articles:\")\n",
        "print(df_large['label'].value_counts())\n",
        "print(\"\\nSample articles:\")\n",
        "print(\"Real news examples:\")\n",
        "for i in range(2):\n",
        "    print(f\"- {df_large[df_large['label']==0].iloc[i]['text']}\")\n",
        "\n",
        "print(\"\\nFake news examples:\")\n",
        "for i in range(2):\n",
        "    print(f\"- {df_large[df_large['label']==1].iloc[i]['text']}\")\n",
        "\n",
        "# Apply preprocessing to large dataset\n",
        "print(\"\\nApplying preprocessing to large dataset...\")\n",
        "df_large['cleaned_text'] = df_large['text'].apply(simple_preprocessor.preprocess_pipeline)\n",
        "\n",
        "print(f\"Preprocessing completed.\")\n",
        "print(f\"Average text length - Original: {df_large['text'].str.len().mean():.1f}, Cleaned: {df_large['cleaned_text'].str.len().mean():.1f}\")\n",
        "\n",
        "# Prepare data for training\n",
        "X_large = df_large['cleaned_text']\n",
        "y_large = df_large['label']\n",
        "\n",
        "# Split data (70% train, 30% test)\n",
        "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
        "    X_large, y_large, test_size=0.3, random_state=42, stratify=y_large\n",
        ")\n",
        "\n",
        "print(f\"\\nLarge dataset splits:\")\n",
        "print(f\"Training set: {len(X_train_large)} samples\")\n",
        "print(f\"Test set: {len(X_test_large)} samples\")\n",
        "print(f\"Training label distribution: {y_train_large.value_counts().to_dict()}\")\n",
        "print(f\"Test label distribution: {y_test_large.value_counts().to_dict()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDADP6eP9_dt",
        "outputId": "ece3f347-1f77-4eed-ed25-dbc6c27d7c5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating larger synthetic dataset...\n",
            "Large dataset created with 1000 articles:\n",
            "label\n",
            "1    500\n",
            "0    500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample articles:\n",
            "Real news examples:\n",
            "- A study conducted by MIT revealed important insights.\n",
            "- Dr. Smith stated that artificial intelligence requires careful consideration.\n",
            "\n",
            "Fake news examples:\n",
            "- EXCLUSIVE REPORT: Government agencies hiding cure for serious illness!\n",
            "- AMAZING BREAKTHROUGH: Simple home remedy reverses aging process!\n",
            "\n",
            "Applying preprocessing to large dataset...\n",
            "Preprocessing completed.\n",
            "Average text length - Original: 66.5, Cleaned: 59.9\n",
            "\n",
            "Large dataset splits:\n",
            "Training set: 700 samples\n",
            "Test set: 300 samples\n",
            "Training label distribution: {1: 350, 0: 350}\n",
            "Test label distribution: {1: 150, 0: 150}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models on the larger dataset with enhanced feature engineering\n",
        "class AdvancedFakeNewsDetector:\n",
        "    def __init__(self):\n",
        "        self.vectorizers = {}\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def extract_linguistic_features(self, texts):\n",
        "        \"\"\"Extract linguistic and stylistic features\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for text in texts:\n",
        "            feature_dict = {}\n",
        "\n",
        "            # Basic text statistics\n",
        "            feature_dict['text_length'] = len(text)\n",
        "            feature_dict['word_count'] = len(text.split())\n",
        "            feature_dict['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0\n",
        "\n",
        "            # Punctuation analysis\n",
        "            feature_dict['exclamation_count'] = text.count('!')\n",
        "            feature_dict['question_count'] = text.count('?')\n",
        "            feature_dict['comma_count'] = text.count(',')\n",
        "            feature_dict['period_count'] = text.count('.')\n",
        "\n",
        "            # Capital letters (from original text)\n",
        "            original_text = text  # Assume we have original text\n",
        "            feature_dict['capital_ratio'] = sum(1 for c in original_text if c.isupper()) / len(original_text) if original_text else 0\n",
        "\n",
        "            # Suspicious word patterns\n",
        "            suspicious_words = ['shocking', 'exclusive', 'breaking', 'urgent', 'amazing', 'secret', 'revealed',\n",
        "                              'bombshell', 'crisis', 'miracle', 'incredible', 'forbidden', 'hidden', 'conspiracy']\n",
        "            feature_dict['suspicious_word_count'] = sum(1 for word in suspicious_words if word in text.lower())\n",
        "\n",
        "            # Emotional language indicators\n",
        "            emotional_words = ['furious', 'angry', 'outraged', 'shocked', 'amazed', 'incredible', 'unbelievable']\n",
        "            feature_dict['emotional_word_count'] = sum(1 for word in emotional_words if word in text.lower())\n",
        "\n",
        "            features.append(feature_dict)\n",
        "\n",
        "        return pd.DataFrame(features)\n",
        "\n",
        "    def create_features(self, X_train, X_test):\n",
        "        \"\"\"Create multiple feature representations\"\"\"\n",
        "        feature_sets = {}\n",
        "\n",
        "        # 1. TF-IDF Features (enhanced)\n",
        "        print(\"Creating enhanced TF-IDF features...\")\n",
        "        tfidf = TfidfVectorizer(\n",
        "            max_features=5000,\n",
        "            ngram_range=(1, 3),\n",
        "            stop_words='english',\n",
        "            min_df=2,\n",
        "            max_df=0.95,\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "        X_test_tfidf = tfidf.transform(X_test)\n",
        "        feature_sets['tfidf'] = (X_train_tfidf, X_test_tfidf)\n",
        "        self.vectorizers['tfidf'] = tfidf\n",
        "\n",
        "        # 2. Count Features with character n-grams\n",
        "        print(\"Creating Count features with character n-grams...\")\n",
        "        count = CountVectorizer(\n",
        "            max_features=3000,\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words='english',\n",
        "            analyzer='word'\n",
        "        )\n",
        "        X_train_count = count.fit_transform(X_train)\n",
        "        X_test_count = count.transform(X_test)\n",
        "        feature_sets['count'] = (X_train_count, X_test_count)\n",
        "        self.vectorizers['count'] = count\n",
        "\n",
        "        # 3. Character-level TF-IDF\n",
        "        print(\"Creating character-level TF-IDF features...\")\n",
        "        char_tfidf = TfidfVectorizer(\n",
        "            max_features=2000,\n",
        "            analyzer='char',\n",
        "            ngram_range=(3, 5),\n",
        "            lowercase=True\n",
        "        )\n",
        "        X_train_char = char_tfidf.fit_transform(X_train)\n",
        "        X_test_char = char_tfidf.transform(X_test)\n",
        "        feature_sets['char_tfidf'] = (X_train_char, X_test_char)\n",
        "        self.vectorizers['char_tfidf'] = char_tfidf\n",
        "\n",
        "        return feature_sets\n",
        "\n",
        "    def train_models(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train multiple ML models with hyperparameter tuning\"\"\"\n",
        "\n",
        "        # Create feature sets\n",
        "        feature_sets = self.create_features(X_train, X_test)\n",
        "\n",
        "        # Define models with better parameters\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(\n",
        "                random_state=42,\n",
        "                max_iter=2000,\n",
        "                C=1.0,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'Naive Bayes': MultinomialNB(alpha=0.1),\n",
        "            'Random Forest': RandomForestClassifier(\n",
        "                n_estimators=200,\n",
        "                random_state=42,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'SVM': SVC(\n",
        "                kernel='linear',\n",
        "                random_state=42,\n",
        "                C=1.0,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(\n",
        "                n_estimators=200,\n",
        "                random_state=42,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Train and evaluate models for each feature set\n",
        "        for feature_name, (X_tr, X_te) in feature_sets.items():\n",
        "            print(f\"\\n--- Training models with {feature_name.upper()} features ---\")\n",
        "            print(f\"Feature matrix shape: {X_tr.shape}\")\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                print(f\"Training {model_name}...\")\n",
        "\n",
        "                try:\n",
        "                    # Train model\n",
        "                    model.fit(X_tr, y_train)\n",
        "\n",
        "                    # Predictions\n",
        "                    y_pred = model.predict(X_te)\n",
        "                    y_pred_proba = model.predict_proba(X_te) if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                    # Store results\n",
        "                    key = f\"{model_name}_{feature_name}\"\n",
        "                    self.results[key] = {\n",
        "                        'model': model,\n",
        "                        'accuracy': accuracy,\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'f1': f1,\n",
        "                        'predictions': y_pred,\n",
        "                        'probabilities': y_pred_proba\n",
        "                    }\n",
        "\n",
        "                    print(f\"  Accuracy: {accuracy:.3f}\")\n",
        "                    print(f\"  Precision: {precision:.3f}\")\n",
        "                    print(f\"  Recall: {recall:.3f}\")\n",
        "                    print(f\"  F1-Score: {f1:.3f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error training {model_name}: {str(e)}\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def get_best_models(self, top_k=5):\n",
        "        \"\"\"Get top performing models\"\"\"\n",
        "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['f1'], reverse=True)\n",
        "        return sorted_results[:top_k]\n",
        "\n",
        "# Initialize and train advanced detector\n",
        "print(\"Training advanced fake news detection models...\")\n",
        "advanced_detector = AdvancedFakeNewsDetector()\n",
        "advanced_results = advanced_detector.train_models(\n",
        "    X_train_large, X_test_large, y_train_large, y_test_large\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhWYJsTD-Bg9",
        "outputId": "c287f74d-afaa-4ada-eb6a-bbc90bd4ea32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training advanced fake news detection models...\n",
            "Creating enhanced TF-IDF features...\n",
            "Creating Count features with character n-grams...\n",
            "Creating character-level TF-IDF features...\n",
            "\n",
            "--- Training models with TFIDF features ---\n",
            "Feature matrix shape: (700, 527)\n",
            "Training Logistic Regression...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Naive Bayes...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Random Forest...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training SVM...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Gradient Boosting...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "\n",
            "--- Training models with COUNT features ---\n",
            "Feature matrix shape: (700, 366)\n",
            "Training Logistic Regression...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Naive Bayes...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Random Forest...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training SVM...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Gradient Boosting...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "\n",
            "--- Training models with CHAR_TFIDF features ---\n",
            "Feature matrix shape: (700, 2000)\n",
            "Training Logistic Regression...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Naive Bayes...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Random Forest...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training SVM...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n",
            "Training Gradient Boosting...\n",
            "  Accuracy: 1.000\n",
            "  Precision: 1.000\n",
            "  Recall: 1.000\n",
            "  F1-Score: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The perfect accuracy suggests overfitting due to synthetic data patterns\n",
        "# Let's analyze the results and create evaluation metrics\n",
        "\n",
        "print(\"=== FAKE NEWS DETECTION MODEL RESULTS ===\")\n",
        "print(\"\\nTop performing models:\")\n",
        "\n",
        "best_models = advanced_detector.get_best_models(10)\n",
        "for i, (model_name, results) in enumerate(best_models[:5]):\n",
        "    print(f\"{i+1}. {model_name}\")\n",
        "    print(f\"   Accuracy: {results['accuracy']:.3f}\")\n",
        "    print(f\"   Precision: {results['precision']:.3f}\")\n",
        "    print(f\"   Recall: {results['recall']:.3f}\")\n",
        "    print(f\"   F1-Score: {results['f1']:.3f}\")\n",
        "    print()\n",
        "\n",
        "# Create confusion matrix for best model\n",
        "best_model_name, best_results = best_models[0]\n",
        "y_pred_best = best_results['predictions']\n",
        "\n",
        "print(f\"Detailed Analysis for Best Model: {best_model_name}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_large, y_pred_best)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"Predicted:  Real  Fake\")\n",
        "print(f\"Real:      [{cm[0,0]:4d} {cm[0,1]:4d}]\")\n",
        "print(f\"Fake:      [{cm[1,0]:4d} {cm[1,1]:4d}]\")\n",
        "print()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Detailed Classification Report:\")\n",
        "print(classification_report(y_test_large, y_pred_best, target_names=['Real News', 'Fake News']))\n",
        "\n",
        "# Feature importance analysis (for models that support it)\n",
        "if 'Random Forest' in best_model_name or 'Gradient Boosting' in best_model_name:\n",
        "    model = best_results['model']\n",
        "    feature_names = None\n",
        "\n",
        "    if 'tfidf' in best_model_name:\n",
        "        feature_names = advanced_detector.vectorizers['tfidf'].get_feature_names_out()\n",
        "    elif 'count' in best_model_name:\n",
        "        feature_names = advanced_detector.vectorizers['count'].get_feature_names_out()\n",
        "\n",
        "    if hasattr(model, 'feature_importances_') and feature_names is not None:\n",
        "        importances = model.feature_importances_\n",
        "        top_indices = np.argsort(importances)[-20:]  # Top 20 features\n",
        "\n",
        "        print(\"\\nTop 20 Most Important Features:\")\n",
        "        print(\"-\" * 40)\n",
        "        for i, idx in enumerate(reversed(top_indices)):\n",
        "            print(f\"{i+1:2d}. {feature_names[idx]:20s} ({importances[idx]:.4f})\")\n",
        "\n",
        "# Error analysis - show misclassified examples (if any)\n",
        "misclassified = np.where(y_test_large != y_pred_best)[0]\n",
        "if len(misclassified) > 0:\n",
        "    print(f\"\\nMisclassified Examples ({len(misclassified)} total):\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, idx in enumerate(misclassified[:5]):  # Show first 5 misclassified\n",
        "        actual_idx = X_test_large.index[idx]\n",
        "        actual_label = \"Real\" if y_test_large.iloc[idx] == 0 else \"Fake\"\n",
        "        predicted_label = \"Real\" if y_pred_best[idx] == 0 else \"Fake\"\n",
        "        text = df_large.loc[actual_idx, 'text']\n",
        "        print(f\"{i+1}. Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "        print(f\"   Text: {text[:100]}...\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"\\nNo misclassified examples - perfect classification!\")\n",
        "\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Total test samples: {len(y_test_large)}\")\n",
        "print(f\"Correctly classified: {np.sum(y_test_large == y_pred_best)}\")\n",
        "print(f\"Misclassified: {np.sum(y_test_large != y_pred_best)}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_large, y_pred_best):.1%}\")\n",
        "\n",
        "# Save results to a summary\n",
        "results_summary = []\n",
        "for model_name, results in advanced_detector.results.items():\n",
        "    results_summary.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': results['accuracy'],\n",
        "        'Precision': results['precision'],\n",
        "        'Recall': results['recall'],\n",
        "        'F1_Score': results['f1']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_summary).sort_values('F1_Score', ascending=False)\n",
        "print(f\"\\n\\nComplete Results Summary:\")\n",
        "print(results_df.to_string(index=False, float_format='%.3f'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55N37cJc-DJG",
        "outputId": "918e123f-5278-4216-93e9-cf49adf0b0f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FAKE NEWS DETECTION MODEL RESULTS ===\n",
            "\n",
            "Top performing models:\n",
            "1. Logistic Regression_tfidf\n",
            "   Accuracy: 1.000\n",
            "   Precision: 1.000\n",
            "   Recall: 1.000\n",
            "   F1-Score: 1.000\n",
            "\n",
            "2. Naive Bayes_tfidf\n",
            "   Accuracy: 1.000\n",
            "   Precision: 1.000\n",
            "   Recall: 1.000\n",
            "   F1-Score: 1.000\n",
            "\n",
            "3. Random Forest_tfidf\n",
            "   Accuracy: 1.000\n",
            "   Precision: 1.000\n",
            "   Recall: 1.000\n",
            "   F1-Score: 1.000\n",
            "\n",
            "4. SVM_tfidf\n",
            "   Accuracy: 1.000\n",
            "   Precision: 1.000\n",
            "   Recall: 1.000\n",
            "   F1-Score: 1.000\n",
            "\n",
            "5. Gradient Boosting_tfidf\n",
            "   Accuracy: 1.000\n",
            "   Precision: 1.000\n",
            "   Recall: 1.000\n",
            "   F1-Score: 1.000\n",
            "\n",
            "Detailed Analysis for Best Model: Logistic Regression_tfidf\n",
            "==================================================\n",
            "Confusion Matrix:\n",
            "Predicted:  Real  Fake\n",
            "Real:      [ 150    0]\n",
            "Fake:      [   0  150]\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Real News       1.00      1.00      1.00       150\n",
            "   Fake News       1.00      1.00      1.00       150\n",
            "\n",
            "    accuracy                           1.00       300\n",
            "   macro avg       1.00      1.00      1.00       300\n",
            "weighted avg       1.00      1.00      1.00       300\n",
            "\n",
            "\n",
            "No misclassified examples - perfect classification!\n",
            "\n",
            "Model Performance Summary:\n",
            "------------------------------\n",
            "Total test samples: 300\n",
            "Correctly classified: 300\n",
            "Misclassified: 0\n",
            "Accuracy: 100.0%\n",
            "\n",
            "\n",
            "Complete Results Summary:\n",
            "                         Model  Accuracy  Precision  Recall  F1_Score\n",
            "     Logistic Regression_tfidf     1.000      1.000   1.000     1.000\n",
            "             Naive Bayes_tfidf     1.000      1.000   1.000     1.000\n",
            "           Random Forest_tfidf     1.000      1.000   1.000     1.000\n",
            "                     SVM_tfidf     1.000      1.000   1.000     1.000\n",
            "       Gradient Boosting_tfidf     1.000      1.000   1.000     1.000\n",
            "     Logistic Regression_count     1.000      1.000   1.000     1.000\n",
            "             Naive Bayes_count     1.000      1.000   1.000     1.000\n",
            "           Random Forest_count     1.000      1.000   1.000     1.000\n",
            "                     SVM_count     1.000      1.000   1.000     1.000\n",
            "       Gradient Boosting_count     1.000      1.000   1.000     1.000\n",
            "Logistic Regression_char_tfidf     1.000      1.000   1.000     1.000\n",
            "        Naive Bayes_char_tfidf     1.000      1.000   1.000     1.000\n",
            "      Random Forest_char_tfidf     1.000      1.000   1.000     1.000\n",
            "                SVM_char_tfidf     1.000      1.000   1.000     1.000\n",
            "  Gradient Boosting_char_tfidf     1.000      1.000   1.000     1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a comprehensive implementation with practical utility functions\n",
        "print(\"Creating Complete Fake News Detection Implementation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class CompleteFakeNewsDetector:\n",
        "    \"\"\"\n",
        "    A complete fake news detection system with preprocessing, training, and prediction capabilities\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.preprocessor = SimpleTextPreprocessor()\n",
        "        self.vectorizer = None\n",
        "        self.model = None\n",
        "        self.is_trained = False\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Preprocess a single text sample\"\"\"\n",
        "        return self.preprocessor.preprocess_pipeline(text)\n",
        "\n",
        "    def extract_text_features(self, text):\n",
        "        \"\"\"Extract stylistic and linguistic features from text\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Basic statistics\n",
        "        features['length'] = len(text)\n",
        "        features['word_count'] = len(text.split())\n",
        "        features['avg_word_length'] = np.mean([len(w) for w in text.split()]) if text.split() else 0\n",
        "\n",
        "        # Punctuation analysis\n",
        "        features['exclamation_marks'] = text.count('!')\n",
        "        features['question_marks'] = text.count('?')\n",
        "        features['all_caps_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
        "\n",
        "        # Suspicious patterns\n",
        "        suspicious_keywords = ['shocking', 'exclusive', 'breaking', 'urgent', 'amazing', 'secret',\n",
        "                              'revealed', 'bombshell', 'crisis', 'miracle', 'you wont believe']\n",
        "        features['suspicious_words'] = sum(1 for word in suspicious_keywords if word in text.lower())\n",
        "\n",
        "        emotional_keywords = ['furious', 'outraged', 'incredible', 'unbelievable', 'devastating']\n",
        "        features['emotional_words'] = sum(1 for word in emotional_keywords if word in text.lower())\n",
        "\n",
        "        return features\n",
        "\n",
        "    def train(self, texts, labels, model_type='logistic_regression'):\n",
        "        \"\"\"Train the fake news detection model\"\"\"\n",
        "        print(f\"Training {model_type} model on {len(texts)} samples...\")\n",
        "\n",
        "        # Preprocess texts\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "\n",
        "        # Create TF-IDF vectorizer\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=3000,\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words='english',\n",
        "            min_df=2,\n",
        "            max_df=0.8\n",
        "        )\n",
        "\n",
        "        # Transform texts to feature vectors\n",
        "        X = self.vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "        # Initialize model\n",
        "        if model_type == 'logistic_regression':\n",
        "            self.model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "        elif model_type == 'naive_bayes':\n",
        "            self.model = MultinomialNB(alpha=0.1)\n",
        "        elif model_type == 'random_forest':\n",
        "            self.model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        elif model_type == 'svm':\n",
        "            self.model = SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced')\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "        # Train model\n",
        "        self.model.fit(X, labels)\n",
        "        self.is_trained = True\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_pred = self.model.predict(X)\n",
        "        train_accuracy = accuracy_score(labels, train_pred)\n",
        "        print(f\"Training completed. Training accuracy: {train_accuracy:.3f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict if a single text is fake news\"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained before making predictions\")\n",
        "\n",
        "        # Preprocess text\n",
        "        processed_text = self.preprocess_text(text)\n",
        "\n",
        "        # Vectorize\n",
        "        X = self.vectorizer.transform([processed_text])\n",
        "\n",
        "        # Predict\n",
        "        prediction = self.model.predict(X)[0]\n",
        "        probability = self.model.predict_proba(X)[0]\n",
        "\n",
        "        # Extract features for analysis\n",
        "        features = self.extract_text_features(text)\n",
        "\n",
        "        return {\n",
        "            'prediction': 'fake' if prediction == 1 else 'real',\n",
        "            'confidence': max(probability),\n",
        "            'fake_probability': probability[1],\n",
        "            'real_probability': probability[0],\n",
        "            'features': features\n",
        "        }\n",
        "\n",
        "    def predict_batch(self, texts):\n",
        "        \"\"\"Predict multiple texts at once\"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained before making predictions\")\n",
        "\n",
        "        # Preprocess texts\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "\n",
        "        # Vectorize\n",
        "        X = self.vectorizer.transform(processed_texts)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.model.predict(X)\n",
        "        probabilities = self.model.predict_proba(X)\n",
        "\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            results.append({\n",
        "                'text': text[:100] + '...' if len(text) > 100 else text,\n",
        "                'prediction': 'fake' if predictions[i] == 1 else 'real',\n",
        "                'confidence': max(probabilities[i]),\n",
        "                'fake_probability': probabilities[i][1]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate(self, test_texts, test_labels):\n",
        "        \"\"\"Evaluate model performance on test data\"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained before evaluation\")\n",
        "\n",
        "        # Preprocess and vectorize test texts\n",
        "        processed_texts = [self.preprocess_text(text) for text in test_texts]\n",
        "        X_test = self.vectorizer.transform(processed_texts)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        y_prob = self.model.predict_proba(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(test_labels, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred, average='weighted')\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_prob\n",
        "        }\n",
        "\n",
        "# Create and train the complete detector\n",
        "detector = CompleteFakeNewsDetector()\n",
        "\n",
        "# Train on our dataset\n",
        "detector.train(\n",
        "    texts=X_train_large.values,\n",
        "    labels=y_train_large.values,\n",
        "    model_type='logistic_regression'\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = detector.evaluate(X_test_large.values, y_test_large.values)\n",
        "\n",
        "print(\"\\n=== MODEL EVALUATION RESULTS ===\")\n",
        "print(f\"Test Accuracy: {test_results['accuracy']:.3f}\")\n",
        "print(f\"Test Precision: {test_results['precision']:.3f}\")\n",
        "print(f\"Test Recall: {test_results['recall']:.3f}\")\n",
        "print(f\"Test F1-Score: {test_results['f1_score']:.3f}\")\n",
        "\n",
        "# Test with new examples\n",
        "print(\"\\n=== TESTING WITH NEW EXAMPLES ===\")\n",
        "\n",
        "new_examples = [\n",
        "    \"The Federal Reserve announced interest rate changes following economic analysis by financial experts.\",\n",
        "    \"SHOCKING: This one weird trick will make you lose 50 pounds overnight - doctors hate it!\",\n",
        "    \"Researchers at Stanford published a peer-reviewed study on renewable energy efficiency.\",\n",
        "    \"BREAKING: Secret government conspiracy exposed - they don't want you to know this truth!\",\n",
        "    \"The company reported quarterly earnings growth in line with market expectations.\"\n",
        "]\n",
        "\n",
        "for text in new_examples:\n",
        "    result = detector.predict(text)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {result['prediction'].upper()} (confidence: {result['confidence']:.3f})\")\n",
        "    print(f\"Fake probability: {result['fake_probability']:.3f}\")\n",
        "\n",
        "    # Show relevant features\n",
        "    features = result['features']\n",
        "    print(f\"Features: {features['suspicious_words']} suspicious words, {features['exclamation_marks']} exclamation marks\")\n",
        "\n",
        "print(\"\\n=== IMPLEMENTATION COMPLETE ===\")\n",
        "print(\"The fake news detection system has been successfully built and tested!\")\n",
        "print(\"Key components implemented:\")\n",
        "print(\"- Text preprocessing pipeline\")\n",
        "print(\"- TF-IDF feature extraction\")\n",
        "print(\"- Multiple ML classifiers (Logistic Regression, Naive Bayes, Random Forest, SVM)\")\n",
        "print(\"- Model evaluation and validation\")\n",
        "print(\"- Real-time prediction capability\")\n",
        "print(\"- Feature analysis for interpretability\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx0seMsd-E7c",
        "outputId": "23950805-f0b1-4156-b254-671e2b38b25f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Complete Fake News Detection Implementation\n",
            "============================================================\n",
            "Training logistic_regression model on 700 samples...\n",
            "Training completed. Training accuracy: 1.000\n",
            "\n",
            "=== MODEL EVALUATION RESULTS ===\n",
            "Test Accuracy: 1.000\n",
            "Test Precision: 1.000\n",
            "Test Recall: 1.000\n",
            "Test F1-Score: 1.000\n",
            "\n",
            "=== TESTING WITH NEW EXAMPLES ===\n",
            "\n",
            "Text: The Federal Reserve announced interest rate changes following economic analysis by financial experts.\n",
            "Prediction: REAL (confidence: 0.692)\n",
            "Fake probability: 0.308\n",
            "Features: 0 suspicious words, 0 exclamation marks\n",
            "\n",
            "Text: SHOCKING: This one weird trick will make you lose 50 pounds overnight - doctors hate it!\n",
            "Prediction: FAKE (confidence: 0.828)\n",
            "Fake probability: 0.828\n",
            "Features: 1 suspicious words, 1 exclamation marks\n",
            "\n",
            "Text: Researchers at Stanford published a peer-reviewed study on renewable energy efficiency.\n",
            "Prediction: REAL (confidence: 0.750)\n",
            "Fake probability: 0.250\n",
            "Features: 0 suspicious words, 0 exclamation marks\n",
            "\n",
            "Text: BREAKING: Secret government conspiracy exposed - they don't want you to know this truth!\n",
            "Prediction: FAKE (confidence: 0.895)\n",
            "Fake probability: 0.895\n",
            "Features: 2 suspicious words, 1 exclamation marks\n",
            "\n",
            "Text: The company reported quarterly earnings growth in line with market expectations.\n",
            "Prediction: REAL (confidence: 0.797)\n",
            "Fake probability: 0.203\n",
            "Features: 0 suspicious words, 0 exclamation marks\n",
            "\n",
            "=== IMPLEMENTATION COMPLETE ===\n",
            "The fake news detection system has been successfully built and tested!\n",
            "Key components implemented:\n",
            "- Text preprocessing pipeline\n",
            "- TF-IDF feature extraction\n",
            "- Multiple ML classifiers (Logistic Regression, Naive Bayes, Random Forest, SVM)\n",
            "- Model evaluation and validation\n",
            "- Real-time prediction capability\n",
            "- Feature analysis for interpretability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30d9c8b6",
        "outputId": "9782e676-143a-4e15-a1b0-0f14372953d7"
      },
      "source": [
        "!pip install -U kaleido"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: choreographer>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.0.9)\n",
            "Requirement already satisfied: logistro>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from kaleido) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.11/dist-packages (from kaleido) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kaleido) (25.0)\n",
            "Requirement already satisfied: simplejson>=3.19.3 in /usr/local/lib/python3.11/dist-packages (from choreographer>=1.0.5->kaleido) (3.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# Data from the provided JSON\n",
        "data = {\n",
        "    \"classifiers\": [\"Logistic Regression\", \"Naive Bayes\", \"Random Forest\", \"SVM\", \"Gradient Boosting\"],\n",
        "    \"accuracy\": [1.000, 1.000, 1.000, 1.000, 1.000],\n",
        "    \"precision\": [1.000, 1.000, 1.000, 1.000, 1.000],\n",
        "    \"recall\": [1.000, 1.000, 1.000, 1.000, 1.000],\n",
        "    \"f1_score\": [1.000, 1.000, 1.000, 1.000, 1.000]\n",
        "}\n",
        "\n",
        "# Abbreviated classifier names to fit 15 character limit\n",
        "abbreviated_classifiers = [\"Logistic Reg\", \"Naive Bayes\", \"Random Forest\", \"SVM\", \"Gradient Boost\"]\n",
        "\n",
        "# Brand colors for the 4 metrics\n",
        "colors = [\"#1FB8CD\", \"#DB4545\", \"#2E8B57\", \"#5D878F\"]\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add bars for each metric\n",
        "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
        "metric_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "\n",
        "for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
        "    fig.add_trace(go.Bar(\n",
        "        name=label,\n",
        "        x=abbreviated_classifiers,\n",
        "        y=data[metric],\n",
        "        marker_color=colors[i],\n",
        "        text=[\"1.00\" if j == 0 else \"\" for j in range(len(data[metric]))],  # Only show text on first classifier to reduce clutter\n",
        "        textposition='outside',\n",
        "        cliponaxis=False\n",
        "    ))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title=\"Fake News Detection Performance\",\n",
        "    xaxis_title=\"Classifier\",\n",
        "    yaxis_title=\"Score\",\n",
        "    barmode='group',\n",
        "    yaxis=dict(range=[0, 1.1]),  # Extend range slightly to accommodate text labels\n",
        "    legend=dict(orientation='h', yanchor='bottom', y=1.05, xanchor='center', x=0.5)\n",
        ")\n",
        "\n",
        "# Display the chart without saving\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ObNENlwy-GcD",
        "outputId": "7acaddc8-31a2-4597-8b24-85fd85595e29"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d855c952-b9b8-450a-bb77-9659eb178a20\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d855c952-b9b8-450a-bb77-9659eb178a20\")) {                    Plotly.newPlot(                        \"d855c952-b9b8-450a-bb77-9659eb178a20\",                        [{\"cliponaxis\":false,\"marker\":{\"color\":\"#1FB8CD\"},\"name\":\"Accuracy\",\"text\":[\"1.00\",\"\",\"\",\"\",\"\"],\"textposition\":\"outside\",\"x\":[\"Logistic Reg\",\"Naive Bayes\",\"Random Forest\",\"SVM\",\"Gradient Boost\"],\"y\":[1.0,1.0,1.0,1.0,1.0],\"type\":\"bar\"},{\"cliponaxis\":false,\"marker\":{\"color\":\"#DB4545\"},\"name\":\"Precision\",\"text\":[\"1.00\",\"\",\"\",\"\",\"\"],\"textposition\":\"outside\",\"x\":[\"Logistic Reg\",\"Naive Bayes\",\"Random Forest\",\"SVM\",\"Gradient Boost\"],\"y\":[1.0,1.0,1.0,1.0,1.0],\"type\":\"bar\"},{\"cliponaxis\":false,\"marker\":{\"color\":\"#2E8B57\"},\"name\":\"Recall\",\"text\":[\"1.00\",\"\",\"\",\"\",\"\"],\"textposition\":\"outside\",\"x\":[\"Logistic Reg\",\"Naive Bayes\",\"Random Forest\",\"SVM\",\"Gradient Boost\"],\"y\":[1.0,1.0,1.0,1.0,1.0],\"type\":\"bar\"},{\"cliponaxis\":false,\"marker\":{\"color\":\"#5D878F\"},\"name\":\"F1-Score\",\"text\":[\"1.00\",\"\",\"\",\"\",\"\"],\"textposition\":\"outside\",\"x\":[\"Logistic Reg\",\"Naive Bayes\",\"Random Forest\",\"SVM\",\"Gradient Boost\"],\"y\":[1.0,1.0,1.0,1.0,1.0],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Score\"},\"range\":[0,1.1]},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.05,\"xanchor\":\"center\",\"x\":0.5},\"title\":{\"text\":\"Fake News Detection Performance\"},\"xaxis\":{\"title\":{\"text\":\"Classifier\"}},\"barmode\":\"group\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d855c952-b9b8-450a-bb77-9659eb178a20');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "# Create the confusion matrix data\n",
        "# Rows: True labels (Real News, Fake News)\n",
        "# Columns: Predicted labels (Real News, Fake News)\n",
        "z = [[150, 0],    # True Real News row\n",
        "     [0, 150]]    # True Fake News row\n",
        "\n",
        "# Create text annotations for the cells\n",
        "text = [['150', '0'],\n",
        "        ['0', '150']]\n",
        "\n",
        "# Create the heatmap\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=z,\n",
        "    text=text,\n",
        "    texttemplate=\"%{text}\",\n",
        "    textfont={\"size\": 20},\n",
        "    colorscale='Blues',\n",
        "    showscale=True,\n",
        "    colorbar=dict(title=\"Count\")\n",
        "))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title=\"Fake News Detection: Confusion Matrix\",\n",
        "    xaxis_title=\"Predicted\",\n",
        "    yaxis_title=\"True Labels\"\n",
        ")\n",
        "\n",
        "# Set axis labels\n",
        "fig.update_xaxes(\n",
        "    tickvals=[0, 1],\n",
        "    ticktext=[\"Real News\", \"Fake News\"]\n",
        ")\n",
        "\n",
        "fig.update_yaxes(\n",
        "    tickvals=[0, 1],\n",
        "    ticktext=[\"Real News\", \"Fake News\"],\n",
        "    autorange='reversed'  # This puts Real News at the top\n",
        ")\n",
        "\n",
        "# Display the chart instead of saving\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fQ8K58Cb-Jgb",
        "outputId": "fc00bad1-1ffe-4ef8-ca23-e34fae567056"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"ceb70272-22fd-4208-a199-ae02faab87b9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ceb70272-22fd-4208-a199-ae02faab87b9\")) {                    Plotly.newPlot(                        \"ceb70272-22fd-4208-a199-ae02faab87b9\",                        [{\"colorbar\":{\"title\":{\"text\":\"Count\"}},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"showscale\":true,\"text\":[[\"150\",\"0\"],[\"0\",\"150\"]],\"textfont\":{\"size\":20},\"texttemplate\":\"%{text}\",\"z\":[[150,0],[0,150]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Fake News Detection: Confusion Matrix\"},\"xaxis\":{\"title\":{\"text\":\"Predicted\"},\"tickvals\":[0,1],\"ticktext\":[\"Real News\",\"Fake News\"]},\"yaxis\":{\"title\":{\"text\":\"True Labels\"},\"tickvals\":[0,1],\"ticktext\":[\"Real News\",\"Fake News\"],\"autorange\":\"reversed\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ceb70272-22fd-4208-a199-ae02faab87b9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}